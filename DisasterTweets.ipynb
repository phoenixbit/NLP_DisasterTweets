{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing of Necessary Versions from Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.24.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.24.4)\n",
      "Requirement already satisfied: pandas==1.4.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.4.4)\n",
      "Requirement already satisfied: scikit-learn==1.5.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (1.5.2)\n",
      "Requirement already satisfied: nltk==3.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (3.7)\n",
      "Requirement already satisfied: tensorflow==2.15.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (2.15.0)\n",
      "Requirement already satisfied: transformers==4.16.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.16.2)\n",
      "Requirement already satisfied: matplotlib==3.8.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (3.8.0)\n",
      "Requirement already satisfied: seaborn==0.11.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.11.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/phoenix/Library/Python/3.10/lib/python/site-packages (from pandas==1.4.4->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas==1.4.4->-r requirements.txt (line 2)) (2023.3.post1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn==1.5.2->-r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn==1.5.2->-r requirements.txt (line 3)) (1.11.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn==1.5.2->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk==3.7->-r requirements.txt (line 4)) (4.67.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk==3.7->-r requirements.txt (line 4)) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk==3.7->-r requirements.txt (line 4)) (2024.11.6)\n",
      "Requirement already satisfied: tensorflow-macos==2.15.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow==2.15.0->-r requirements.txt (line 5)) (2.15.0)\n",
      "Requirement already satisfied: sacremoses in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.16.2->-r requirements.txt (line 6)) (0.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/phoenix/Library/Python/3.10/lib/python/site-packages (from transformers==4.16.2->-r requirements.txt (line 6)) (23.1)\n",
      "Requirement already satisfied: requests in /Users/phoenix/Library/Python/3.10/lib/python/site-packages (from transformers==4.16.2->-r requirements.txt (line 6)) (2.31.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.16.2->-r requirements.txt (line 6)) (0.26.3)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.16.2->-r requirements.txt (line 6)) (3.16.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.16.2->-r requirements.txt (line 6)) (0.21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/phoenix/Library/Python/3.10/lib/python/site-packages (from transformers==4.16.2->-r requirements.txt (line 6)) (6.0.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib==3.8.0->-r requirements.txt (line 7)) (4.43.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib==3.8.0->-r requirements.txt (line 7)) (3.1.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib==3.8.0->-r requirements.txt (line 7)) (9.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib==3.8.0->-r requirements.txt (line 7)) (0.12.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib==3.8.0->-r requirements.txt (line 7)) (1.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib==3.8.0->-r requirements.txt (line 7)) (1.4.5)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (2.0.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (2.15.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (3.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (1.59.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (63.2.0)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (23.5.26)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (2.15.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (16.0.6)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/phoenix/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (2.15.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (2.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (4.25.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (3.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (4.3.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (1.14.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (0.37.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2->-r requirements.txt (line 6)) (2024.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers==4.16.2->-r requirements.txt (line 6)) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers==4.16.2->-r requirements.txt (line 6)) (2.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers==4.16.2->-r requirements.txt (line 6)) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers==4.16.2->-r requirements.txt (line 6)) (2022.6.15)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (0.41.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (3.0.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (2.23.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (3.5.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (0.7.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (0.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (5.3.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/phoenix/Library/Python/3.10/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow==2.15.0->-r requirements.txt (line 5)) (3.2.2)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pip/_vendor/rich/console.py\", line 1673, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pip/_vendor/rich/console.py\", line 1305, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pip/_vendor/rich/segment.py\", line 249, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pip/_vendor/rich/console.py\", line 1283, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/Library/Frameworks/Python.framework/Versions/3.10/bin/pip'\n",
      "Call stack:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pip/__main__.py\", line 31, in <module>\n",
      "    sys.exit(_main())\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 190, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pip/_internal/self_outdated_check.py\", line 236, in pip_self_version_check\n",
      "    logger.warning(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/logging/__init__.py\", line 1489, in warning\n",
      "    self._log(WARNING, msg, args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
      "    self.handle(record)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
      "    self.emit(record)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.2.2', new='24.3.1'),)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/phoenix/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/phoenix/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/phoenix/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/phoenix/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# For Basic Data Handling and Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For Text Processing\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import urllib.parse\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "# For Neural Networks \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# For Transformers and Pre-Trained Models\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "\n",
    "# For Model Evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# For Data Visualization\n",
    "import matplotlib.pyplot as plt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Training and Testing Data and Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          5080\n",
       "keyword     5080\n",
       "location    5080\n",
       "text        5080\n",
       "target      5080\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "train = pd.read_csv(\"./nlp-getting-started/train.csv\") \n",
    "\n",
    "test = pd.read_csv(\"./nlp-getting-started/test.csv\")\n",
    "\n",
    "# Drop N/A values\n",
    "train.dropna(inplace=True)\n",
    "\n",
    "test.dropna(inplace=True)\n",
    "\n",
    "# Get total count of rows\n",
    "train.count()\n",
    "\n",
    "### Current count is 5080 post removal of N/A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the text\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # Make text in lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove Punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation.replace(\"'\", \"\")))  \n",
    "    # arguments of str.maketrans(to_be_replaced, replacing, to_be_deleted)\n",
    "    # string.punctuation is a predefined function that contains all punctuations like !, #, ;, etc.\n",
    "    # the .replace for the same is used to remove the apostrophe from the list of punctuations.\n",
    "    # So, in the end text will 1) have nothing to replace, 2) replace with nothing, 3) delete all punctuations except \"'\".\n",
    "\n",
    "    # Tokenizing the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Removing the stop words\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "    # Choosing between stemming and lemmatizing based on case, here Lemmatizing will be better.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens) # used to get a single string of tokens separated by spaces \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')\n",
      "    id keyword                       location  \\\n",
      "31  48  ablaze                     Birmingham   \n",
      "32  49  ablaze  Est. September 2012 - Bristol   \n",
      "33  50  ablaze                         AFRICA   \n",
      "34  52  ablaze               Philadelphia, PA   \n",
      "35  53  ablaze                     London, UK   \n",
      "\n",
      "                                                 text  target  \n",
      "31  @bbcmtd Wholesale Markets ablaze http://t.co/l...       1  \n",
      "32  We always try to bring the heavy. #metal #RT h...       0  \n",
      "33  #AFRICANBAZE: Breaking news:Nigeria flag set a...       1  \n",
      "34                 Crying out for more! Set me ablaze       0  \n",
      "35  On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0  \n"
     ]
    }
   ],
   "source": [
    "print(train.columns)\n",
    "\n",
    "print(train[:5])\n",
    "\n",
    "#printing rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning function to all of the necessary columns\n",
    "train['keyword_new'] = train['keyword'].apply(clean_text)\n",
    "test['keyword_new'] = test['keyword'].apply(clean_text)\n",
    "\n",
    "train['location_new'] = train['location'].apply(clean_text)\n",
    "test['location_new'] = test['location'].apply(clean_text)\n",
    "\n",
    "train['text_new'] = train['text'].apply(clean_text)\n",
    "test['text_new'] = test['text'].apply(clean_text)\n",
    "\n",
    "\n",
    "# Decoding the keywords\n",
    "train['keyword'] = train['keyword'].apply(urllib.parse.unquote)\n",
    "test['keyword'] = test['keyword'].apply(urllib.parse.unquote)\n",
    "\n",
    "\n",
    "# Prepare features and target for modeling\n",
    "features = train[['keyword_new', 'location_new', 'text_new']]  \n",
    "target = train['target']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, target, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'keyword', 'location', 'text', 'target', 'keyword_new',\n",
      "       'location_new', 'text_new'],\n",
      "      dtype='object')\n",
      "    id keyword                       location  \\\n",
      "31  48  ablaze                     Birmingham   \n",
      "32  49  ablaze  Est. September 2012 - Bristol   \n",
      "33  50  ablaze                         AFRICA   \n",
      "34  52  ablaze               Philadelphia, PA   \n",
      "35  53  ablaze                     London, UK   \n",
      "\n",
      "                                                 text  target keyword_new  \\\n",
      "31  @bbcmtd Wholesale Markets ablaze http://t.co/l...       1      ablaze   \n",
      "32  We always try to bring the heavy. #metal #RT h...       0      ablaze   \n",
      "33  #AFRICANBAZE: Breaking news:Nigeria flag set a...       1      ablaze   \n",
      "34                 Crying out for more! Set me ablaze       0      ablaze   \n",
      "35  On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0      ablaze   \n",
      "\n",
      "                  location_new  \\\n",
      "31                  birmingham   \n",
      "32  est september 2012 bristol   \n",
      "33                      africa   \n",
      "34             philadelphia pa   \n",
      "35                   london uk   \n",
      "\n",
      "                                             text_new  \n",
      "31   bbcmtd wholesale market ablaze httptcolhyxeohy6c  \n",
      "32  always try bring heavy metal rt httptcoyao1e0xngw  \n",
      "33  africanbaze breaking newsnigeria flag set abla...  \n",
      "34                                     cry set ablaze  \n",
      "35  plus side look sky last night ablaze httptcoqq...  \n"
     ]
    }
   ],
   "source": [
    "print(train.columns)\n",
    "\n",
    "print(train[:5])\n",
    "\n",
    "# printing rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=400)\n",
    "# Initializing the vectorizer with max_features set to 1) 400\n",
    "\n",
    "# Combine all text data into one corpus for fitting\n",
    "combined_train_text = X_train['keyword_new'] + ' ' + X_train['location_new'] + ' ' + X_train['text_new']\n",
    "vectorizer.fit(combined_train_text)  # Fit once on the combined corpus\n",
    "\n",
    "# Transform each set separately without refitting\n",
    "tfidf_k_train = vectorizer.transform(X_train['keyword_new']).toarray()\n",
    "tfidf_l_train = vectorizer.transform(X_train['location_new']).toarray()\n",
    "tfidf_t_train = vectorizer.transform(X_train['text_new']).toarray()\n",
    "\n",
    "tfidf_k_val = vectorizer.transform(X_val['keyword_new']).toarray()\n",
    "tfidf_l_val = vectorizer.transform(X_val['location_new']).toarray()\n",
    "tfidf_t_val = vectorizer.transform(X_val['text_new']).toarray()\n",
    "\n",
    "tfidf_k_test = vectorizer.transform(test['keyword_new']).toarray()\n",
    "tfidf_l_test = vectorizer.transform(test['location_new']).toarray()\n",
    "tfidf_t_test = vectorizer.transform(test['text_new']).toarray()\n",
    "\n",
    "# Concatenate transformed data for each set\n",
    "X_train_tfidf = np.concatenate((tfidf_k_train, tfidf_l_train, tfidf_t_train), axis=1)\n",
    "X_val_tfidf = np.concatenate((tfidf_k_val, tfidf_l_val, tfidf_t_val), axis=1)\n",
    "X_test_tfidf = np.concatenate((tfidf_k_test, tfidf_l_test, tfidf_t_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique keywords: 221\n"
     ]
    }
   ],
   "source": [
    "# Check unique values in the 'keyword' column\n",
    "unique_keywords = train['keyword'].unique()\n",
    "\n",
    "# Count of unique keywords\n",
    "unique_keyword_count = len(unique_keywords)\n",
    "\n",
    "print(f\"Number of unique keywords: {unique_keyword_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ablaze' 'accident' 'aftershock' 'airplane accident' 'ambulance'\n",
      " 'annihilated' 'annihilation' 'apocalypse' 'armageddon' 'army' 'arson'\n",
      " 'arsonist' 'attack' 'attacked' 'avalanche' 'battle' 'bioterror'\n",
      " 'bioterrorism' 'blaze' 'blazing' 'bleeding' 'blew up' 'blight' 'blizzard'\n",
      " 'blood' 'bloody' 'blown up' 'body bag' 'body bagging' 'body bags' 'bomb'\n",
      " 'bombed' 'bombing' 'bridge collapse' 'buildings burning'\n",
      " 'buildings on fire' 'burned' 'burning' 'burning buildings' 'bush fires'\n",
      " 'casualties' 'casualty' 'catastrophe' 'catastrophic' 'chemical emergency'\n",
      " 'cliff fall' 'collapse' 'collapsed' 'collide' 'collided' 'collision'\n",
      " 'crash' 'crashed' 'crush' 'crushed' 'curfew' 'cyclone' 'damage' 'danger'\n",
      " 'dead' 'death' 'deaths' 'debris' 'deluge' 'deluged' 'demolish'\n",
      " 'demolished' 'demolition' 'derail' 'derailed' 'derailment' 'desolate'\n",
      " 'desolation' 'destroy' 'destroyed' 'destruction' 'detonate' 'detonation'\n",
      " 'devastated' 'devastation' 'disaster' 'displaced' 'drought' 'drown'\n",
      " 'drowned' 'drowning' 'dust storm' 'earthquake' 'electrocute'\n",
      " 'electrocuted' 'emergency' 'emergency plan' 'emergency services'\n",
      " 'engulfed' 'epicentre' 'evacuate' 'evacuated' 'evacuation' 'explode'\n",
      " 'exploded' 'explosion' 'eyewitness' 'famine' 'fatal' 'fatalities'\n",
      " 'fatality' 'fear' 'fire' 'fire truck' 'first responders' 'flames'\n",
      " 'flattened' 'flood' 'flooding' 'floods' 'forest fire' 'forest fires'\n",
      " 'hail' 'hailstorm' 'harm' 'hazard' 'hazardous' 'heat wave' 'hellfire'\n",
      " 'hijack' 'hijacker' 'hijacking' 'hostage' 'hostages' 'hurricane'\n",
      " 'injured' 'injuries' 'injury' 'inundated' 'inundation' 'landslide' 'lava'\n",
      " 'lightning' 'loud bang' 'mass murder' 'mass murderer' 'massacre' 'mayhem'\n",
      " 'meltdown' 'military' 'mudslide' 'natural disaster' 'nuclear disaster'\n",
      " 'nuclear reactor' 'obliterate' 'obliterated' 'obliteration' 'oil spill'\n",
      " 'outbreak' 'pandemonium' 'panic' 'panicking' 'police' 'quarantine'\n",
      " 'quarantined' 'radiation emergency' 'rainstorm' 'razed' 'refugees'\n",
      " 'rescue' 'rescued' 'rescuers' 'riot' 'rioting' 'rubble' 'ruin'\n",
      " 'sandstorm' 'screamed' 'screaming' 'screams' 'seismic' 'sinkhole'\n",
      " 'sinking' 'siren' 'sirens' 'smoke' 'snowstorm' 'storm' 'stretcher'\n",
      " 'structural failure' 'suicide bomb' 'suicide bomber' 'suicide bombing'\n",
      " 'sunk' 'survive' 'survived' 'survivors' 'terrorism' 'terrorist' 'threat'\n",
      " 'thunder' 'thunderstorm' 'tornado' 'tragedy' 'trapped' 'trauma'\n",
      " 'traumatised' 'trouble' 'tsunami' 'twister' 'typhoon' 'upheaval'\n",
      " 'violent storm' 'volcano' 'war zone' 'weapon' 'weapons' 'whirlwind'\n",
      " 'wild fires' 'wildfire' 'windstorm' 'wounded' 'wounds' 'wreck' 'wreckage'\n",
      " 'wrecked']\n",
      "collision              36\n",
      "whirlwind              33\n",
      "armageddon             32\n",
      "fatalities             32\n",
      "flames                 31\n",
      "                       ..\n",
      "razed                  10\n",
      "epicentre               9\n",
      "detonation              8\n",
      "radiation emergency     6\n",
      "inundation              5\n",
      "Name: keyword, Length: 221, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print unique keywords\n",
    "print(unique_keywords)\n",
    "\n",
    "# Frequency of each keyword\n",
    "keyword_frequency = train['keyword'].value_counts()\n",
    "print(keyword_frequency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7667322834645669\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.83      0.80       580\n",
      "           1       0.75      0.69      0.72       436\n",
      "\n",
      "    accuracy                           0.77      1016\n",
      "   macro avg       0.76      0.76      0.76      1016\n",
      "weighted avg       0.77      0.77      0.77      1016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenating the training, validation and test TF-IDF vectors for the features that had TF-IDF embeddings \n",
    "X_train_tfidf = np.concatenate((tfidf_k_train, tfidf_l_train, tfidf_t_train), axis=1)\n",
    "X_test_tfidf = np.concatenate((tfidf_k_test, tfidf_l_test, tfidf_t_test), axis=1)\n",
    "X_val_tfidf = np.concatenate((tfidf_k_val, tfidf_l_val, tfidf_t_val), axis=1)\n",
    "\n",
    "# Implementing a new Log Regression Model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = model.predict(X_val_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_val, predictions))\n",
    "print(classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.7667322834645669\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.80      0.80       580\n",
      "           1       0.73      0.72      0.73       436\n",
      "\n",
      "    accuracy                           0.77      1016\n",
      "   macro avg       0.76      0.76      0.76      1016\n",
      "weighted avg       0.77      0.77      0.77      1016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "nb_predictions = nb_classifier.predict(X_val_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_val, nb_predictions))\n",
    "print(classification_report(y_val, nb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
